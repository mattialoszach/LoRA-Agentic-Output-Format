{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12434732,
          "sourceType": "datasetVersion",
          "datasetId": 7843617
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b47c7dc12a7404087109869207322f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0a14a695c4c43c8a2171d04ab6970f5",
              "IPY_MODEL_ccb5483fcf5c4e898c14cc4660bd55ff",
              "IPY_MODEL_9dba12e9f07546f0b8c6f1d4c4bfe752"
            ],
            "layout": "IPY_MODEL_527a50f7fdc447a79b0fd84013e51cfa"
          }
        },
        "d0a14a695c4c43c8a2171d04ab6970f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c305978be7094738b39d01dc628a87ad",
            "placeholder": "​",
            "style": "IPY_MODEL_1053453865f344fda20f62a29638ef50",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "ccb5483fcf5c4e898c14cc4660bd55ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da10e2312a554cfa85e8d3a648252ec2",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f06b0eb95584ea09b16fb5f95646dab",
            "value": 300
          }
        },
        "9dba12e9f07546f0b8c6f1d4c4bfe752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccdf8a888f9241b688a4d477e81bfaa8",
            "placeholder": "​",
            "style": "IPY_MODEL_1db570b17c384ef5bd192798a67bf938",
            "value": " 300/300 [00:00&lt;00:00, 2556.21 examples/s]"
          }
        },
        "527a50f7fdc447a79b0fd84013e51cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c305978be7094738b39d01dc628a87ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1053453865f344fda20f62a29638ef50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da10e2312a554cfa85e8d3a648252ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f06b0eb95584ea09b16fb5f95646dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccdf8a888f9241b688a4d477e81bfaa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1db570b17c384ef5bd192798a67bf938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y unsloth peft\n",
        "!pip install unsloth trl peft accelerate bitsandbytes\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T20:48:26.089784Z",
          "iopub.execute_input": "2025-07-10T20:48:26.090449Z",
          "iopub.status.idle": "2025-07-10T20:48:32.212938Z",
          "shell.execute_reply.started": "2025-07-10T20:48:26.090408Z",
          "shell.execute_reply": "2025-07-10T20:48:32.212031Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhIuFho3evI3",
        "outputId": "b0ac461b-5410-4237-ab94-8e53faa7f84e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: unsloth 2025.7.3\n",
            "Uninstalling unsloth-2025.7.3:\n",
            "  Successfully uninstalled unsloth-2025.7.3\n",
            "Found existing installation: peft 0.16.0\n",
            "Uninstalling peft-0.16.0:\n",
            "  Successfully uninstalled peft-0.16.0\n",
            "Collecting unsloth\n",
            "  Using cached unsloth-2025.7.3-py3-none-any.whl.metadata (47 kB)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
            "Collecting peft\n",
            "  Using cached peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.7.4 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.7.4)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.7.1)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.31.post1)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.26)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.53.1)\n",
            "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.34.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.22.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (1.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (0.21.2)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.4->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.4->unsloth) (11.2.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.4->unsloth) (0.19.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.7.9)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Using cached unsloth-2025.7.3-py3-none-any.whl (297 kB)\n",
            "Using cached peft-0.16.0-py3-none-any.whl (472 kB)\n",
            "Installing collected packages: peft, unsloth\n",
            "Successfully installed peft-0.16.0 unsloth-2025.7.3\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T21:22:44.741099Z",
          "iopub.execute_input": "2025-07-10T21:22:44.741929Z",
          "iopub.status.idle": "2025-07-10T21:22:44.747006Z",
          "shell.execute_reply.started": "2025-07-10T21:22:44.741900Z",
          "shell.execute_reply": "2025-07-10T21:22:44.746320Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvppZp8pevI4",
        "outputId": "d8518b6e-dd66-4f6f-d0d3-5dc879ba8c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load new data"
      ],
      "metadata": {
        "id": "yyLK0ZR_evI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV -> JSON\n",
        "df = pd.read_csv(\"/content/it_support_agent_dataset_300.csv\")\n",
        "df[\"output\"] = df[\"output\"].apply(eval)\n",
        "data = df.to_dict(orient=\"records\")\n",
        "with open(\"/content/it_support_agent_dataset_300.json\", \"w\") as f:\n",
        "    json.dump(data, f, indent=2)\n",
        "print(data[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gel4RqvREbUX",
        "outputId": "49caada5-3d2c-4c70-8d9d-11dda7359409"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'The printer is not responding.', 'output': {'action': 'run_diagnostics', 'query': 'The printer is not responding.', 'result': {'status': 'resolved', 'details': \"Action 'run_diagnostics' executed for issue: The printer is not responding.\"}, 'reasoning': \"The system detected an issue requiring 'run_diagnostics' to address: 'The printer is not responding.'\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Base Model & Tokenizer"
      ],
      "metadata": {
        "id": "MOnAJAqmevI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/Qwen3-1.7B\" # Choose pre-trained model\n",
        "max_seq_length = 2048  # Choose sequence length\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "# Model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T21:14:20.910638Z",
          "iopub.execute_input": "2025-07-10T21:14:20.911322Z",
          "iopub.status.idle": "2025-07-10T21:14:26.707000Z",
          "shell.execute_reply.started": "2025-07-10T21:14:20.911298Z",
          "shell.execute_reply": "2025-07-10T21:14:26.706419Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hhlFTIBevI5",
        "outputId": "d9bbdf59-d172-45cd-8bed-3e2e530a29f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.3: Fast Qwen3 patching. Transformers: 4.53.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Training Prompts"
      ],
      "metadata": {
        "id": "aLHngkYlevI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(example):\n",
        "    return (\n",
        "        f\"### Input: {example['input']}\\n\"\n",
        "        f\"### Output (in JSON):\\n{json.dumps(example['output'], indent=2)}\\n<|endoftext|>\"\n",
        "    )\n",
        "\n",
        "formatted_data = [format_prompt(item) for item in data]\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T21:16:52.936002Z",
          "iopub.execute_input": "2025-07-10T21:16:52.936385Z",
          "iopub.status.idle": "2025-07-10T21:16:52.968958Z",
          "shell.execute_reply.started": "2025-07-10T21:16:52.936340Z",
          "shell.execute_reply": "2025-07-10T21:16:52.968377Z"
        },
        "id": "y2aPByHxevI6"
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inject LoRA Adapters into Model"
      ],
      "metadata": {
        "id": "umlEasVdevI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,  # LoRA rank -> higher = more capacity, more memory\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
        "    lora_dropout=0,  # Optimized for speed\n",
        "    bias=\"none\",     # Optimized for speed\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
        "    random_state=4242,\n",
        "    use_rslora=False,  # Rank stabilized LoRA\n",
        "    loftq_config=None, # LoftQ\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T21:22:14.181781Z",
          "iopub.execute_input": "2025-07-10T21:22:14.182547Z",
          "iopub.status.idle": "2025-07-10T21:22:21.622231Z",
          "shell.execute_reply.started": "2025-07-10T21:22:14.182522Z",
          "shell.execute_reply": "2025-07-10T21:22:21.621569Z"
        },
        "id": "jlAKmZ78evI6"
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Trainer\n",
        "This performs the Fine-Tuning"
      ],
      "metadata": {
        "id": "CeGBq8wkevI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=4242,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        dataloader_pin_memory=False,\n",
        "        report_to=None,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T21:24:50.046919Z",
          "iopub.execute_input": "2025-07-10T21:24:50.047226Z",
          "iopub.status.idle": "2025-07-10T21:24:50.878717Z",
          "shell.execute_reply.started": "2025-07-10T21:24:50.047206Z",
          "shell.execute_reply": "2025-07-10T21:24:50.878114Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "5b47c7dc12a7404087109869207322f8",
            "d0a14a695c4c43c8a2171d04ab6970f5",
            "ccb5483fcf5c4e898c14cc4660bd55ff",
            "9dba12e9f07546f0b8c6f1d4c4bfe752",
            "527a50f7fdc447a79b0fd84013e51cfa",
            "c305978be7094738b39d01dc628a87ad",
            "1053453865f344fda20f62a29638ef50",
            "da10e2312a554cfa85e8d3a648252ec2",
            "6f06b0eb95584ea09b16fb5f95646dab",
            "ccdf8a888f9241b688a4d477e81bfaa8",
            "1db570b17c384ef5bd192798a67bf938"
          ]
        },
        "id": "PZJ5bh4EevI6",
        "outputId": "18c66b5c-22e2-4140-b4f6-cd0142558a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/300 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b47c7dc12a7404087109869207322f8"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "wMZHzN9aevI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T21:26:55.154443Z",
          "iopub.execute_input": "2025-07-10T21:26:55.155157Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "1u-nzriQevI6",
        "outputId": "696c0896-65f1-4fd3-8ad3-1b4965abdabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 300 | Num Epochs = 3 | Total steps = 114\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 69,730,304 of 1,790,305,280 (3.89% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [114/114 03:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.566700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.132600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.116000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.105400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.103200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.101500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.098400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.098000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.095100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.094000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "sEINsadQevI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frOAgoSWevI7",
        "outputId": "1df50e86-0574-4db5-ecd5-1b7dbb6f7f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 1.4G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 3.25 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:01<00:00, 21.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving gguf_model/pytorch_model.bin...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting qwen3 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at gguf_model into f16 GGUF format.\n",
            "The output location will be /content/gguf_model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: gguf_model\n",
            "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 40960\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 6144\n",
            "INFO:hf-to-gguf:gguf: head count = 16\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151645\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- messages[0].content + '\\n\\n' }}\n",
            "    {%- endif %}\n",
            "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
            "{%- for forward_message in messages %}\n",
            "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
            "    {%- set message = messages[index] %}\n",
            "    {%- set tool_start = '<tool_response>' %}\n",
            "    {%- set tool_start_length = tool_start|length %}\n",
            "    {%- set start_of_message = message.content[:tool_start_length] %}\n",
            "    {%- set tool_end = '</tool_response>' %}\n",
            "    {%- set tool_end_length = tool_end|length %}\n",
            "    {%- set start_pos = (message.content|length) - tool_end_length %}\n",
            "    {%- if start_pos < 0 %}\n",
            "        {%- set start_pos = 0 %}\n",
            "    {%- endif %}\n",
            "    {%- set end_of_message = message.content[start_pos:] %}\n",
            "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
            "        {%- set ns.multi_step_tool = false %}\n",
            "        {%- set ns.last_query_index = index %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {%- set content = message.content %}\n",
            "        {%- set reasoning_content = '' %}\n",
            "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
            "            {%- set reasoning_content = message.reasoning_content %}\n",
            "        {%- else %}\n",
            "            {%- if '</think>' in message.content %}\n",
            "                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n",
            "                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n",
            "                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
            "            {%- endif %}\n",
            "        {%- endif %}\n",
            "        {%- if loop.index0 > ns.last_query_index %}\n",
            "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
            "            {%- else %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "            {%- endif %}\n",
            "        {%- else %}\n",
            "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "        {%- endif %}\n",
            "        {%- if message.tool_calls %}\n",
            "            {%- for tool_call in message.tool_calls %}\n",
            "                {%- if (loop.first and content) or (not loop.first) %}\n",
            "                    {{- '\\n' }}\n",
            "                {%- endif %}\n",
            "                {%- if tool_call.function %}\n",
            "                    {%- set tool_call = tool_call.function %}\n",
            "                {%- endif %}\n",
            "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
            "                {{- tool_call.name }}\n",
            "                {{- '\", \"arguments\": ' }}\n",
            "                {%- if tool_call.arguments is string %}\n",
            "                    {{- tool_call.arguments }}\n",
            "                {%- else %}\n",
            "                    {{- tool_call.arguments | tojson }}\n",
            "                {%- endif %}\n",
            "                {{- '}\\n</tool_call>' }}\n",
            "            {%- endfor %}\n",
            "        {%- endif %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
            "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/gguf_model/unsloth.F16.gguf: n_tensors = 310, total_size = 3.4G\n",
            "Writing: 100%|██████████| 3.44G/3.44G [01:24<00:00, 40.8Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 5914 (b0f0ecc3)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/gguf_model/unsloth.F16.gguf' to '/content/gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 310 tensors from /content/gguf_model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gguf_Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 1.7B\n",
            "llama_model_loader: - kv   4:                          qwen3.block_count u32              = 28\n",
            "llama_model_loader: - kv   5:                       qwen3.context_length u32              = 40960\n",
            "llama_model_loader: - kv   6:                     qwen3.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   7:                  qwen3.feed_forward_length u32              = 6144\n",
            "llama_model_loader: - kv   8:                 qwen3.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   9:              qwen3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                 qwen3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               qwen3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - type  f32:  113 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 310]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 310]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[   3/ 310]                  blk.0.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[   4/ 310]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   5/ 310]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 310]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   7/ 310]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   8/ 310]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   9/ 310]                  blk.0.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  10/ 310]                blk.0.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  11/ 310]                blk.0.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  12/ 310]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 310]                  blk.0.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  14/ 310]                  blk.1.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  15/ 310]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  16/ 310]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  17/ 310]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  18/ 310]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  19/ 310]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  20/ 310]                  blk.1.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  21/ 310]                blk.1.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  22/ 310]                blk.1.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  23/ 310]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  24/ 310]                  blk.1.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  25/ 310]                  blk.2.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  26/ 310]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  27/ 310]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  28/ 310]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  29/ 310]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  30/ 310]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  31/ 310]                  blk.2.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  32/ 310]                blk.2.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  33/ 310]                blk.2.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  34/ 310]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  35/ 310]                  blk.2.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  36/ 310]                  blk.3.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  37/ 310]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  38/ 310]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  39/ 310]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  40/ 310]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  41/ 310]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  42/ 310]                  blk.3.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  43/ 310]                blk.3.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  44/ 310]                blk.3.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  45/ 310]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  46/ 310]                  blk.3.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  47/ 310]                  blk.4.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  48/ 310]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  49/ 310]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 310]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  51/ 310]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  52/ 310]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  53/ 310]                  blk.4.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  54/ 310]                blk.4.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  55/ 310]                blk.4.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  56/ 310]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  57/ 310]                  blk.4.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  58/ 310]                  blk.5.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  59/ 310]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  60/ 310]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  61/ 310]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  62/ 310]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  63/ 310]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  64/ 310]                  blk.5.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  65/ 310]                blk.5.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  66/ 310]                blk.5.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  67/ 310]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 310]                  blk.5.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  69/ 310]                  blk.6.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  70/ 310]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  71/ 310]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  72/ 310]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  73/ 310]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  74/ 310]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  75/ 310]                  blk.6.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  76/ 310]                blk.6.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  77/ 310]                blk.6.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  78/ 310]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  79/ 310]                  blk.6.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  80/ 310]                  blk.7.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  81/ 310]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  82/ 310]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  83/ 310]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  84/ 310]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  85/ 310]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  86/ 310]                  blk.7.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  87/ 310]                blk.7.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  88/ 310]                blk.7.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  89/ 310]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  90/ 310]                  blk.7.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  91/ 310]                  blk.8.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  92/ 310]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  93/ 310]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  94/ 310]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 310]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  96/ 310]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  97/ 310]                  blk.8.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  98/ 310]                blk.8.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  99/ 310]                blk.8.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 100/ 310]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 101/ 310]                  blk.8.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 102/ 310]                  blk.9.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 103/ 310]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 104/ 310]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 105/ 310]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 106/ 310]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 107/ 310]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 108/ 310]                  blk.9.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 109/ 310]                blk.9.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 110/ 310]                blk.9.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 111/ 310]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 310]                  blk.9.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 113/ 310]                 blk.10.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 114/ 310]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 115/ 310]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 310]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 117/ 310]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 118/ 310]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 119/ 310]                 blk.10.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 120/ 310]               blk.10.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 121/ 310]               blk.10.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 122/ 310]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 123/ 310]                 blk.10.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 124/ 310]                 blk.11.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 125/ 310]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 126/ 310]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 127/ 310]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 128/ 310]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 129/ 310]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 130/ 310]                 blk.11.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 131/ 310]               blk.11.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 132/ 310]               blk.11.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 133/ 310]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 310]                 blk.11.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 135/ 310]                 blk.12.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 136/ 310]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 137/ 310]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 138/ 310]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 139/ 310]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 140/ 310]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 141/ 310]                 blk.12.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 142/ 310]               blk.12.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 143/ 310]               blk.12.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 144/ 310]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 145/ 310]                 blk.12.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 146/ 310]                 blk.13.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 147/ 310]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 148/ 310]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 149/ 310]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 150/ 310]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 151/ 310]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 152/ 310]                 blk.13.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 153/ 310]               blk.13.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 154/ 310]               blk.13.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 155/ 310]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 156/ 310]                 blk.13.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 157/ 310]                 blk.14.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 158/ 310]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 159/ 310]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 160/ 310]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 161/ 310]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 310]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 163/ 310]                 blk.14.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 164/ 310]               blk.14.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 165/ 310]               blk.14.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 166/ 310]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 167/ 310]                 blk.14.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 168/ 310]                 blk.15.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 169/ 310]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 170/ 310]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 171/ 310]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 172/ 310]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 173/ 310]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 174/ 310]                 blk.15.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 175/ 310]               blk.15.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 176/ 310]               blk.15.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 177/ 310]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 178/ 310]                 blk.15.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 179/ 310]                 blk.16.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 180/ 310]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 181/ 310]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 310]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 183/ 310]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 184/ 310]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 185/ 310]                 blk.16.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 186/ 310]               blk.16.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 187/ 310]               blk.16.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 188/ 310]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 189/ 310]                 blk.16.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 190/ 310]                 blk.17.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 191/ 310]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 192/ 310]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 193/ 310]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 194/ 310]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 195/ 310]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 196/ 310]                 blk.17.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 197/ 310]               blk.17.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 198/ 310]               blk.17.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 199/ 310]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 310]                 blk.17.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 201/ 310]                 blk.18.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 202/ 310]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 203/ 310]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 204/ 310]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 205/ 310]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 206/ 310]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 207/ 310]                 blk.18.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 208/ 310]               blk.18.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 209/ 310]               blk.18.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 210/ 310]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 211/ 310]                 blk.18.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 212/ 310]                 blk.19.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 213/ 310]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 214/ 310]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 215/ 310]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 310]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 217/ 310]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 218/ 310]                 blk.19.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 219/ 310]               blk.19.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 220/ 310]               blk.19.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 221/ 310]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 222/ 310]                 blk.19.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 223/ 310]                 blk.20.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 224/ 310]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 225/ 310]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 226/ 310]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 227/ 310]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 228/ 310]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 229/ 310]                 blk.20.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 230/ 310]               blk.20.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 231/ 310]               blk.20.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 232/ 310]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 233/ 310]                 blk.20.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 234/ 310]                 blk.21.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 235/ 310]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 236/ 310]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 237/ 310]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 238/ 310]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 310]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 240/ 310]                 blk.21.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 241/ 310]               blk.21.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 242/ 310]               blk.21.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 243/ 310]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 244/ 310]                 blk.21.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 245/ 310]                 blk.22.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 246/ 310]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 247/ 310]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 248/ 310]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 249/ 310]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 250/ 310]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 251/ 310]                 blk.22.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 252/ 310]               blk.22.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 253/ 310]               blk.22.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 254/ 310]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 255/ 310]                 blk.22.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 256/ 310]                 blk.23.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 257/ 310]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 258/ 310]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 259/ 310]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 260/ 310]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 261/ 310]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 262/ 310]                 blk.23.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 263/ 310]               blk.23.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 264/ 310]               blk.23.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 265/ 310]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 266/ 310]                 blk.23.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 267/ 310]                 blk.24.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 268/ 310]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 269/ 310]              blk.24.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 270/ 310]            blk.24.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 271/ 310]                 blk.24.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 272/ 310]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 273/ 310]                 blk.24.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 274/ 310]               blk.24.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 275/ 310]               blk.24.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 276/ 310]               blk.24.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 277/ 310]                 blk.24.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 278/ 310]                 blk.25.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 279/ 310]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 280/ 310]              blk.25.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 281/ 310]            blk.25.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 282/ 310]                 blk.25.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 283/ 310]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 284/ 310]                 blk.25.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 285/ 310]               blk.25.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 286/ 310]               blk.25.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 287/ 310]               blk.25.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 288/ 310]                 blk.25.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 289/ 310]                 blk.26.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 290/ 310]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 291/ 310]              blk.26.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 292/ 310]            blk.26.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 293/ 310]                 blk.26.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 294/ 310]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 295/ 310]                 blk.26.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 296/ 310]               blk.26.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 297/ 310]               blk.26.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 298/ 310]               blk.26.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 299/ 310]                 blk.26.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 300/ 310]                 blk.27.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 301/ 310]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 302/ 310]              blk.27.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 303/ 310]            blk.27.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 304/ 310]                 blk.27.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 305/ 310]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 306/ 310]                 blk.27.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 307/ 310]               blk.27.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 308/ 310]               blk.27.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 309/ 310]               blk.27.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 310/ 310]                 blk.27.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "llama_model_quantize_impl: model size  =  3281.97 MB\n",
            "llama_model_quantize_impl: quant size  =  1050.43 MB\n",
            "\n",
            "main: quantize time = 198770.54 ms\n",
            "main:    total time = 198770.54 ms\n",
            "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "STa7fA4oevI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Provide semantic context (example)\n",
        "ACTION_GUIDE = \"\"\"\n",
        "Actions:\n",
        "-run_diagnostics (printer is not responding)\n",
        "\"\"\"\n",
        "\n",
        "# Test Input\n",
        "user_input = \"No response from the printer.\"\n",
        "\n",
        "# Build Prompt\n",
        "prompt = f\"\"\"### SYSTEM: {ACTION_GUIDE.strip()}\n",
        "\n",
        "### Input: {user_input}\n",
        "### Output (in JSON):\"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=256,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gezLXFievI7",
        "outputId": "1b59ed19-c4b6-4a05-be0c-b5ef16906061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### SYSTEM: Actions:\n",
            "-run_diagnostics (printer is not responding)\n",
            "\n",
            "### Input: No response from the printer.\n",
            "### Output (in JSON): {\n",
            "  \"action\": \"run_diagnostics\",\n",
            "  \"query\": \"No response from the printer.\",\n",
            "  \"result\": {\n",
            "    \"status\": \"escalated\",\n",
            "    \"details\": \"Action 'run_diagnostics' executed for issue: No response from the printer.\"\n",
            "  },\n",
            "  \"reasoning\": \"The system detected an issue requiring 'run_diagnostics' to address: 'No response from the printer.'\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "execution_count": 47
    }
  ]
}